{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The NBA's Next Super Star\n",
    "\n",
    "## Abstract\n",
    "All NBA superstars have their rookie seasons. No matter it started with a [terrible debut year (eg. Lebron James)](http://www.thesportster.com/basketball/15-nba-superstars-who-were-terrible-in-their-debut-season/) or rewarding one like being selected to the [all-star game (eg. Tim Duncan)](http://www.nba-allstar.com/players/lists/all-star-game-rookies.htm), it is believed that there exists some evidences in their early years foretelling their future success. In this project, we will study and examine those key factors that drives the players towards a successful career, and eventually predicting the future super star in NBA.\n",
    "\n",
    "![alt text][all_star_game_2016]\n",
    "\n",
    "[all_star_game_2016]: http://i.cdn.turner.com/nba/nba/dam/assets/160121164737-all-star-starters-graphic-1280-012116.1200x672.jpg \"All Star Game 2016\"\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "A common way to foresee a successful NBA player from his rookie year is normally by looking at the draft order. However, such argument is incomprehensible for not taking other extrinsic factors into account.\n",
    "\n",
    "## Introduction\n",
    "This project aims to predict the career growth of an NBA player with the aid of the following data sources that reflects a player's performance, annual ratings on video games, and exposure on social media and newspaper.\n",
    "\n",
    "1. Performance: The past and current on-court statistics on-court. (eg. [1991 NBA Draft](http://www.basketball-reference.com/draft/NBA_1991.html))\n",
    "\n",
    "2. Ratings: The player ratings in the NBA 2K16 video game (eg. [2K16 MyTEAM Players](http://2kmtcentral.com/16/players/theme/dynamic))\n",
    "\n",
    "3. Social Media: The amount of tweets of a player might reflect his attitude and seriousness of his career. Besides, a language model composed of the tweet content could be useful.(eg. [Twitter API](https://dev.twitter.com/rest/reference/get/statuses/user_timeline))\n",
    "\n",
    "4. Related News: As news carries information such as fans' support, anticipation, expert analysis, injury report, and anticipation, the language model built accordingly would be a great indicator.\n",
    "\n",
    "## Objective\n",
    "Judging from indicators such as efficiency or other stats on-court, we would like to predict the likelihood of a player becoming a superstar that dominates the league in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtains all rookies using function scrape_draft. In order to better predict each player's future performance, their performance in each regualr season should be considered. Therfore, we will crawl NBA player scoring per game on ESPN, and merge with the rookies dataframe. Finally, we will derive a dataframe containing each rookie, and their performance statistics since drafts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def obj2numeric(df, cols, str_cols = ['Tm', 'PLAYER', 'College']):\n",
    "    \"\"\"\n",
    "    convert column type to numeric except those string fields\n",
    "    \n",
    "    Args:\n",
    "    (data_frame): data frame to be converted\n",
    "    \n",
    "    Return:\n",
    "    (data_frame): data frame with numeric fields converted\n",
    "    \"\"\"\n",
    "    for c in cols:\n",
    "        if c not in str_cols: df[c] = pd.to_numeric(df[c], errors='raise')\n",
    "    return df\n",
    "\n",
    "def transform_numeric(df):\n",
    "    \"\"\"\n",
    "    convert columns type to numeric\n",
    "    \n",
    "    Args:\n",
    "    (data_frame): data frame whose columns contain object type\n",
    "    \n",
    "    Return:\n",
    "    (data_frame): data frame with numeric values converted\n",
    "    \"\"\"\n",
    "    df.rename(columns={'WS/48':'WS_per_48', 'Player':'PLAYER'}, inplace=True)\n",
    "    df.columns.values[14:18] = [df.columns.values[14:18][col] + \"_per_game\" for col in range(4)]\n",
    "    df = obj2numeric(df, df.columns)\n",
    "    df = df[df['PLAYER'].notnull()].fillna(0)\n",
    "    df.loc[:,'Yrs':'AST'] = df.loc[:,'Yrs':'AST'].astype(int)\n",
    "    return df\n",
    "    \n",
    "def scrape_draft(save_file, start_yr=1966, end_yr=2016):\n",
    "    \"\"\"\n",
    "    Scrape draft data for the specified duration from:\n",
    "    http://www.basketball-reference.com/draft/NBA_{year}.html\n",
    "    \n",
    "    Args:\n",
    "    start_yr(int): start of year for scraping\n",
    "    end_yr(int): end of year for scraping\n",
    "    \n",
    "    Return:\n",
    "    (data_frame): Annually draft pick result\n",
    "    \"\"\"\n",
    "    url_format = 'http://www.basketball-reference.com/draft/NBA_{yr}.html'\n",
    "    frames = []\n",
    "    \n",
    "    for y in range(start_yr, end_yr):\n",
    "        url = url_format.format(yr = y)\n",
    "        bs = BeautifulSoup(urlopen(url), 'html.parser')\n",
    "        \n",
    "        # columns and remove the header column(Rk)\n",
    "        tr_tags = bs.findAll('tr')\n",
    "        th_tags = tr_tags[1].findAll('th')\n",
    "        \n",
    "        cols = [th.getText() for th in th_tags]; cols.pop(0)\n",
    "        rows = [[td.getText() for td in tr_tags[i].findAll('td')] for i in range(2, len(tr_tags))]\n",
    "\n",
    "        year_df = pd.DataFrame(rows, columns = cols)\n",
    "        year_df.insert(0,'Draft_Yr', y)\n",
    "        frames.append(year_df)\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "    df = transform_numeric(df)\n",
    "    df.to_csv(save_file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see ESPN_nba_player_stats.py for more details \n",
    "from ESPN_nba_player_stats import get_regular_season\n",
    "\n",
    "def merge_draft_espn(draft_file, union_file, start_yr=1990, end_yr=2017):\n",
    "    \"\"\"\n",
    "    merge all rookies from draft with statistics from ESPN\n",
    "    \n",
    "    Args:\n",
    "    (data_frame): data frame to be converted\n",
    "    \n",
    "    Return:\n",
    "    (data_frame): data frame with numeric fields converted\n",
    "    \"\"\"\n",
    "    draft_df = pd.read_csv(draft_file, index_col=0)\n",
    "\n",
    "    for yr in range(start_yr, end_yr):\n",
    "        yr_data = get_regular_season(yr)\n",
    "        # Only cares players in draft_df\n",
    "        draft_df = pd.merge(draft_df, yr_data, on=\"PLAYER\", how=\"left\")\n",
    "\n",
    "    draft_df.to_csv(union_file)\n",
    "    return draft_df\n",
    "\n",
    "start_yr = 1999\n",
    "end_yr = 2017 # exclusive\n",
    "\n",
    "draft_file_format = \"draft_data_{start_yr}_to_{end_yr}.csv\"\n",
    "union_file_format = \"all_data_{start_yr}_to_{end_yr}.csv\"\n",
    "draft_file = draft_file_format.format(start_yr = start_yr, end_yr = end_yr)\n",
    "union_file = union_file_format.format(start_yr = start_yr, end_yr = end_yr)\n",
    "\n",
    "# Retrive 2009 - 2016 rookies, and ESPN regular season statistic from 2009 - 2016\n",
    "draft_df = scrape_draft(draft_file, start_yr, end_yr)\n",
    "all_df = merge_draft_espn(draft_file, union_file, start_yr, end_yr)\n",
    "print all_df.head(10)\n",
    "print all_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with all star stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ref/allstar.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-397cc33e6a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mallstar_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ref/allstar.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mallstar_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallstar_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparateNameField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ref/allstar.csv does not exist"
     ]
    }
   ],
   "source": [
    "def separateNameField(row):\n",
    "    pattern = r'(\\w*[\\'-]*\\w* \\w*[\\'-]*\\w*).*'\n",
    "    s = row['Player']\n",
    "    s = s.replace('\\xd5', '\\'')\n",
    "    m = re.search(pattern, s)\n",
    "    row['Player'] = m.group(1)\n",
    "    return row\n",
    "\n",
    "allstar_stats = pd.read_csv('ref/allstar.csv')\n",
    "allstar_stats = allstar_stats.apply(separateNameField, axis = 1)\n",
    "\n",
    "feature_stats = pd.read_csv('all_data_1999_to_2017.csv')\n",
    "player_name = set(allstar_stats['Player'])\n",
    "\n",
    "\n",
    "features = feature_stats.iloc[:, 0:23]\n",
    "features['allstar'] = False\n",
    "for i, r in features.iterrows():\n",
    "    features.loc[i, 'allstar'] = r['PLAYER'] in player_name\n",
    "    \n",
    "df_college = pd.get_dummies(feature_stats['College'])\n",
    "features = pd.concat([features, df_college], axis=1, join_axes=[features.index])\n",
    "features = features.drop(['Unnamed: 0', 'College'], axis = 1)\n",
    "features.to_csv('features.csv')\n",
    "\n",
    "\n",
    "# TODO: plot university distribution\n",
    "# pd.get_dummies(feature_stats['College']).sum(axis = 0)\n",
    "# feature_stats[feature_stats['allstar'] == True][['PLAYER', 'allstar']]\n",
    "#     feature_stats['allstar'] = feature_stats['PLAYER']\n",
    "# feature_stats['allstar'] = player_name(feature_stats['allstar'])\n",
    "# feature_stats['allstar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Load the diabetes dataset\n",
    "features_all = pd.read_csv('features.csv')\n",
    "features_all = features_all.drop(['Unnamed: 0', 'Tm', 'PLAYER'], axis = 1)\n",
    "\n",
    "y = features_all['allstar']\n",
    "X = features_all.drop('allstar', axis = 1)\n",
    "ind = int(X.shape[0] * 0.7)\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "m, n = X.shape\n",
    "perm = np.random.permutation(m)\n",
    "X, y = X.loc[perm], y.loc[perm]\n",
    "diabetes_X_train = X[:ind]\n",
    "diabetes_X_test = X[ind:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = y[:ind]\n",
    "diabetes_y_test = y[ind:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# # The coefficients\n",
    "# print('Coefficients: \\n', regr.coef_)\n",
    "# # The mean squared error\n",
    "# print(\"Mean squared error: %.2f\"\n",
    "#       % np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2))\n",
    "# # Explained variance score: 1 is perfect prediction\n",
    "# print('Variance score: %.2f' % regr.score(diabetes_X_test, diabetes_y_test))\n",
    "\n",
    "# Plot outputs\n",
    "# print regr.predict(diabetes_X_train).shape\n",
    "# print diabetes_y_train\n",
    "for threshold in range(1, 10, 1):\n",
    "    threshold /= 10.0\n",
    "    print threshold\n",
    "    print ((regr.predict(diabetes_X_train) > threshold)  == diabetes_y_train).mean()\n",
    "    print ((regr.predict(diabetes_X_test) > threshold) == diabetes_y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_ft(df):\n",
    "    int_cols = ['GP', 'total_FTM', 'total_FTA']\n",
    "    df.loc[:,'GP':'FT%'] = df.loc[:,'GP':'FT%'].astype(float)\n",
    "    df[int_cols] = df[int_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "def transform_3p(df):\n",
    "    int_cols = ['GP', '3PA', 'total_3PM', 'total_3PA', '2PM']\n",
    "    df.loc[:,'GP':'FG%'] = df.loc[:,'GP':'FG%'].astype(float)\n",
    "    df[int_cols] = df[int_cols].astype(int)\n",
    "    return df\n",
    "\n",
    "def transform_fg(df):\n",
    "    df.columns = list(df.columns[:-1]) + [('adj_' + df.columns[-1]).decode('utf-8')]\n",
    "    int_cols = ['GP', 'total_FGM', 'total_FGA', '2PM', '2PA']\n",
    "    df.loc[:,'GP':'adj_FG%'] = df.loc[:,'GP':'adj_FG%'].astype(float)\n",
    "    df[int_cols] = df[int_cols].astype(int)\n",
    "    return df\n",
    "    \n",
    "def scrape_ncaa(url, cnt):\n",
    "    bs = BeautifulSoup(urlopen(url), 'html.parser')\n",
    "    tr_tags = bs.findAll('tr')\n",
    "    rows = []\n",
    "    cols = tr_tags[1].findAll('td')\n",
    "    cols = [th.getText() for th in cols]\n",
    "    cols[7:9] = ['total_' + s for s in cols[7:9]]\n",
    "\n",
    "    for i in range(2, len(tr_tags)):\n",
    "        td_tags = tr_tags[i].findAll('td')\n",
    "        rows.append([th.getText() for th in td_tags])\n",
    "    df = pd.DataFrame(rows, columns = cols)\n",
    "    \n",
    "    # Clean up\n",
    "    df = df[df['PLAYER'] != 'PER GAME']\n",
    "    df = df[df['PLAYER'] != 'PLAYER']\n",
    "    df = df.reset_index()\n",
    "    df.drop('index', axis = 1, inplace = True)\n",
    "    df['RK'] = df.index + cnt\n",
    "    return df\n",
    "\n",
    "def scrape_ncaa_year(url):\n",
    "    df_fg_list = []; cnt = 1\n",
    "    for cnt in range(1, 100, 40):\n",
    "        df_fg = scrape_ncaa(url + str(cnt), cnt)\n",
    "        df_fg_list.append(df_fg)\n",
    "    df = pd.concat(df_fg_list)\n",
    "    df = df.reset_index('RK')\n",
    "    df.drop('index', axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "def scrape_ncaa_yr_range(s_yr, e_yr, url):\n",
    "    df_list = []\n",
    "    for yr in range(2002, 2016):\n",
    "        url_reg = url + str(yr) + '/count/'\n",
    "        url_pos = url + str(yr) + '/seasontype/3'\n",
    "        df_list.append(scrape_ncaa_year(url_reg))\n",
    "        df_list.append(scrape_ncaa_year(url_pos))\n",
    "    df = pd.concat(df_list)\n",
    "    df = df.reset_index()\n",
    "    df.drop('index', axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "def scrape_driver(s_yr, e_yr):\n",
    "    # Create urls\n",
    "    url_base = 'http://www.espn.com/mens-college-basketball/statistics/player/_/stat/'\n",
    "    stat_list = ['free-throws', '3-points', 'field-goals']\n",
    "    url_list = [url_base + stat + '/year/' for stat in stat_list]\n",
    "\n",
    "    # Scrape free throw\n",
    "    df_ft = scrape_ncaa_yr_range(s_yr, e_yr, url_list[0])\n",
    "    df_ft = transform_ft(df_ft)\n",
    "    df_ft.to_csv('free_throw.csv')\n",
    "\n",
    "    # Scrape three-point\n",
    "    df_3p = scrape_ncaa_yr_range(2002, 2016, url_list[1])\n",
    "    df_3p = transform_3p(df_3p)\n",
    "    df_3p.to_csv('three_point.csv')\n",
    "\n",
    "    # Scrape field-goal\n",
    "    df_fg = scrape_ncaa_yr_range(2002, 2016, url_list[2])\n",
    "    df_fg = transform_fg(df_fg)\n",
    "    df_fg.to_csv('field_goal.csv')\n",
    "    \n",
    "    return (df_ft, df_3p, df_fg)\n",
    "    \n",
    "(df_ft, df_3p, df_fg) = scrape_driver(2002, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA Pre-draft Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "physical_stats = pd.read_csv('nba-pre-draft-measurements.csv')\n",
    "physical_stats.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Edge case examples:\n",
    "    Shareef O'Neal - 2015\n",
    "    Maxwell Lorca-Lloyd - 2016\n",
    "    Johnny O\\xd5Bryant - 2013\n",
    "    Chris Wright (Gtown) - 2009\n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "pattern = r'\\W*(\\w* \\w*[\\'-]*\\w*).* - (\\d{4})'\n",
    "\n",
    "def separateNameField(row):\n",
    "    s = row['Name']\n",
    "    s = s.replace('\\xd5', '\\'')\n",
    "    m = re.search(pattern, s)\n",
    "    \n",
    "    row['Name'] = m.group(1)\n",
    "    row['DraftYear'] = m.group(2)\n",
    "    return row\n",
    "    \n",
    "physical_stats2 = physical_stats.apply(separateNameField, axis = 1)\n",
    "physical_stats2.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
